
---
title: 20250219周报
date: 2025-02-19 22:15:12
tags:
---

# 周报 20250213~20250219

## 论文阅读及复现

### 正在阅读 Deepseek LLM
[DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954)

#### 学习了 LlaMA 架构
pre-Norm 把归一层放前面(我在复现Transformers架构时好像也这么做了) 

RMSNorm 一种更高效的归一层
与传统的Layer Norm相比，RMSNorm计算起来更加高效，因为它不需要对输入进行减去均值的操作，从而减少了计算量和内存占用。

> 给定一个输入向量\(x\)，RMSNorm的输出可以表示为：
>\[ y = \frac{x}{\sqrt{\mathbb{E}[x^2] + \epsilon}} \cdot \gamma \]
>
>这里：
> - \( \mathbb{E}[x^2] \) 表示输入向量\(x\)各元素平方的平均值。
> - \( \epsilon \) 是一个小的常数，用于防止分母为零的情况，保证数值稳定性。
> - \( \gamma \) 是一个可学习的参数，用于缩放归一化后的结果。

SwiGLU，一种具有门控机制的激活函数，增强模型非线性表达能力。

> SwiGLU(x) = (Swish(xW) ⊙ xV) * b
> Swish(x) = x * sigmoid(βx)

Rotary Embedding 将位置信息编码为旋转矩阵，作用于查询和键向量。
借助了复数的思想，通过绝对位置编码的方式实现了相对位置编码的效果

Grouped-Query Attention 将查询头分组，每组共享相同的键和值头。
可以在比较大的参数量时使用

#### 超参选取实验
>result:
> $ 𝜂_{opt} = 0.3118 \times 𝐶^{−0.1250} $
> $ 𝐵_{opt} = 0.2920 \times 𝐶^{0.3271} $

> 好久不见，复杂度（这里计算了参数量）

> 在增加计算预算时，扩大模型规模比增加数据规模更有优势

### MP5论文复现进度

总结：由于对虚拟机和docker等基本知识不够了解，浪费大量时间，因此几乎没有（

1. 搞定了openai的api
2. minecraft agent的辅助库 minedojo 基于ubunto或macOS 尝试使用虚拟机搭建但不是很成功（虚拟机低手）
3. 目前遇到不知道如何在虚拟机环境搭建代理的问题-平时使用的代理程序没有linux版本，正在寻找解决方案

## 其他

- 背单词
- 超前课堂进度自学操作系统中
- 在自己的虚拟机中尝试使用 rm /* 删除当前文件夹并成功重装（还好是空的虚拟机