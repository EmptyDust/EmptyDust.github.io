<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ml_server_wp | Fengling's Blog</title><meta name="author" content="风铃夜行"><meta name="copyright" content="风铃夜行"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="使用vscode的remote-ssh拓展连接服务器 我不是很确定我是如何搞定这个的，但可能是这样:使用shell将conda加入path使其全局可用 创建名为code的目录，然后在vsc中打开它，就可以在vsc的终端中使用它了。 1mkdir code  123456789101112131415# init.sh# &gt;&gt;&gt; conda initialize &gt;&gt;&amp;">
<meta property="og:type" content="article">
<meta property="og:title" content="ml_server_wp">
<meta property="og:url" content="https://www.emptydust.com/2024/12/28/ml-server-wp/index.html">
<meta property="og:site_name" content="Fengling&#39;s Blog">
<meta property="og:description" content="使用vscode的remote-ssh拓展连接服务器 我不是很确定我是如何搞定这个的，但可能是这样:使用shell将conda加入path使其全局可用 创建名为code的目录，然后在vsc中打开它，就可以在vsc的终端中使用它了。 1mkdir code  123456789101112131415# init.sh# &gt;&gt;&gt; conda initialize &gt;&gt;&amp;">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.helloimg.com/i/2024/12/28/676fa53a9c318.png">
<meta property="article:published_time" content="2024-12-28T02:26:21.000Z">
<meta property="article:modified_time" content="2024-12-28T13:34:26.843Z">
<meta property="article:author" content="风铃夜行">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.helloimg.com/i/2024/12/28/676fa53a9c318.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.emptydust.com/2024/12/28/ml-server-wp/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ml_server_wp',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: false,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://www.helloimg.com/i/2024/12/28/676fa53a9c318.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://www.helloimg.com/i/2024/12/28/676fab4151e1d.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://www.helloimg.com/i/2024/12/28/676fa53a9c318.png" alt="Logo"><span class="site-name">Fengling's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">ml_server_wp</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">ml_server_wp</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-28T02:26:21.000Z" title="发表于 2024-12-28 10:26:21">2024-12-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-28T13:34:26.843Z" title="更新于 2024-12-28 21:34:26">2024-12-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/catag-test/">catag_test</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>使用vscode的remote-ssh拓展连接服务器</p>
<p>我不是很确定我是如何搞定这个的，但可能是这样:<br>使用shell将conda加入path使其全局可用</p>
<p>创建名为code的目录，然后在vsc中打开它，就可以在vsc的终端中使用它了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir code</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">init.sh</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">&gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">!! Contents within this block are managed by <span class="string">&#x27;conda init&#x27;</span> !!</span></span><br><span class="line">__conda_setup=&quot;$(&#x27;/usr/local/miniconda3/bin/conda&#x27; &#x27;shell.bash&#x27; &#x27;hook&#x27; 2&gt; /dev/null)&quot;</span><br><span class="line">if [ $? -eq 0 ]; then</span><br><span class="line">    eval &quot;$__conda_setup&quot;</span><br><span class="line">else</span><br><span class="line">    if [ -f &quot;/usr/local/miniconda3/etc/profile.d/conda.sh&quot; ]; then</span><br><span class="line">        . &quot;/usr/local/miniconda3/etc/profile.d/conda.sh&quot;</span><br><span class="line">    else</span><br><span class="line">        export PATH=&quot;/usr/local/miniconda3/bin:$PATH&quot;</span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line">unset __conda_setup</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">&lt;&lt;&lt; <span class="string">conda initialize &lt;&lt;</span></span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh init.sh</span><br></pre></td></tr></table></figure>

<p>以<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html">某一章节</a>的代码作为例子。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda create --name d2l python=3.9 -y</span><br><span class="line">conda activate d2l</span><br><span class="line">pip install torch==2.0.0 torchvision==0.15.1</span><br><span class="line">pip install d2l==1.0.3</span><br></pre></td></tr></table></figure>

<p>不建议使用jupyter，而是以输出图片到相同文件夹下代替<br>因此相比原文重写了train_ch13函数</p>
<p>运行成功的代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;bert.base&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;bert.base.torch.zip&#x27;</span>,</span><br><span class="line">                             <span class="string">&#x27;225d66f04cae318b841a13d32af3acc165f253ac&#x27;</span>)</span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;bert.small&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;bert.small.torch.zip&#x27;</span>,</span><br><span class="line">                              <span class="string">&#x27;c72329e68a732bef0452e4b96a1c341c8910f81f&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_pretrained_model</span>(<span class="params">pretrained_model, num_hiddens, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                          num_heads, num_blks, dropout, max_len, devices</span>):</span><br><span class="line">    data_dir = d2l.download_extract(pretrained_model)</span><br><span class="line">    <span class="comment"># Define an empty vocabulary to load the predefined vocabulary</span></span><br><span class="line">    vocab = d2l.Vocab()</span><br><span class="line">    vocab.idx_to_token = json.load(<span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;vocab.json&#x27;</span>)))</span><br><span class="line">    vocab.token_to_idx = &#123;token: idx <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">        vocab.idx_to_token)&#125;</span><br><span class="line">    bert = d2l.BERTModel(</span><br><span class="line">        <span class="built_in">len</span>(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=<span class="number">4</span>,</span><br><span class="line">        num_blks=<span class="number">2</span>, dropout=<span class="number">0.2</span>, max_len=max_len)</span><br><span class="line">    <span class="comment"># Load pretrained BERT parameters</span></span><br><span class="line">    bert.load_state_dict(torch.load(os.path.join(data_dir,</span><br><span class="line">                                                 <span class="string">&#x27;pretrained.params&#x27;</span>)))</span><br><span class="line">    <span class="keyword">return</span> bert, vocab</span><br><span class="line"></span><br><span class="line">devices = d2l.try_all_gpus()</span><br><span class="line">bert, vocab = load_pretrained_model(</span><br><span class="line">    <span class="string">&#x27;bert.small&#x27;</span>, num_hiddens=<span class="number">256</span>, ffn_num_hiddens=<span class="number">512</span>, num_heads=<span class="number">4</span>,</span><br><span class="line">    num_blks=<span class="number">2</span>, dropout=<span class="number">0.1</span>, max_len=<span class="number">512</span>, devices=devices)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SNLIBERTDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, max_len, vocab=<span class="literal">None</span></span>):</span><br><span class="line">        all_premise_hypothesis_tokens = [[</span><br><span class="line">            p_tokens, h_tokens] <span class="keyword">for</span> p_tokens, h_tokens <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">            *[d2l.tokenize([s.lower() <span class="keyword">for</span> s <span class="keyword">in</span> sentences])</span><br><span class="line">              <span class="keyword">for</span> sentences <span class="keyword">in</span> dataset[:<span class="number">2</span>]])]</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.labels = torch.tensor(dataset[<span class="number">2</span>])</span><br><span class="line">        <span class="variable language_">self</span>.vocab = vocab</span><br><span class="line">        <span class="variable language_">self</span>.max_len = max_len</span><br><span class="line">        (<span class="variable language_">self</span>.all_token_ids, <span class="variable language_">self</span>.all_segments,</span><br><span class="line">         <span class="variable language_">self</span>.valid_lens) = <span class="variable language_">self</span>._preprocess(all_premise_hypothesis_tokens)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.all_token_ids)) + <span class="string">&#x27; examples&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_preprocess</span>(<span class="params">self, all_premise_hypothesis_tokens</span>):</span><br><span class="line">        pool = multiprocessing.Pool(<span class="number">4</span>)  <span class="comment"># Use 4 worker processes</span></span><br><span class="line">        out = pool.<span class="built_in">map</span>(<span class="variable language_">self</span>._mp_worker, all_premise_hypothesis_tokens)</span><br><span class="line">        all_token_ids = [</span><br><span class="line">            token_ids <span class="keyword">for</span> token_ids, segments, valid_len <span class="keyword">in</span> out]</span><br><span class="line">        all_segments = [segments <span class="keyword">for</span> token_ids, segments, valid_len <span class="keyword">in</span> out]</span><br><span class="line">        valid_lens = [valid_len <span class="keyword">for</span> token_ids, segments, valid_len <span class="keyword">in</span> out]</span><br><span class="line">        <span class="keyword">return</span> (torch.tensor(all_token_ids, dtype=torch.long),</span><br><span class="line">                torch.tensor(all_segments, dtype=torch.long),</span><br><span class="line">                torch.tensor(valid_lens))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_mp_worker</span>(<span class="params">self, premise_hypothesis_tokens</span>):</span><br><span class="line">        p_tokens, h_tokens = premise_hypothesis_tokens</span><br><span class="line">        <span class="variable language_">self</span>._truncate_pair_of_tokens(p_tokens, h_tokens)</span><br><span class="line">        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)</span><br><span class="line">        token_ids = <span class="variable language_">self</span>.vocab[tokens] + [<span class="variable language_">self</span>.vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] \</span><br><span class="line">                             * (<span class="variable language_">self</span>.max_len - <span class="built_in">len</span>(tokens))</span><br><span class="line">        segments = segments + [<span class="number">0</span>] * (<span class="variable language_">self</span>.max_len - <span class="built_in">len</span>(segments))</span><br><span class="line">        valid_len = <span class="built_in">len</span>(tokens)</span><br><span class="line">        <span class="keyword">return</span> token_ids, segments, valid_len</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_truncate_pair_of_tokens</span>(<span class="params">self, p_tokens, h_tokens</span>):</span><br><span class="line">        <span class="comment"># Reserve slots for &#x27;&lt;CLS&gt;&#x27;, &#x27;&lt;SEP&gt;&#x27;, and &#x27;&lt;SEP&gt;&#x27; tokens for the BERT</span></span><br><span class="line">        <span class="comment"># input</span></span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(p_tokens) + <span class="built_in">len</span>(h_tokens) &gt; <span class="variable language_">self</span>.max_len - <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(p_tokens) &gt; <span class="built_in">len</span>(h_tokens):</span><br><span class="line">                p_tokens.pop()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                h_tokens.pop()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="variable language_">self</span>.all_token_ids[idx], <span class="variable language_">self</span>.all_segments[idx],</span><br><span class="line">                <span class="variable language_">self</span>.valid_lens[idx]), <span class="variable language_">self</span>.labels[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.all_token_ids)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reduce `batch_size` if there is an out of memory error. In the original BERT</span></span><br><span class="line"><span class="comment"># model, `max_len` = 512</span></span><br><span class="line">batch_size, max_len, num_workers = <span class="number">512</span>, <span class="number">128</span>, d2l.get_dataloader_workers()</span><br><span class="line">data_dir = d2l.download_extract(<span class="string">&#x27;SNLI&#x27;</span>)</span><br><span class="line">train_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class="literal">True</span>), max_len, vocab)</span><br><span class="line">test_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class="literal">False</span>), max_len, vocab)</span><br><span class="line">train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                   num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(test_set, batch_size,</span><br><span class="line">                                  num_workers=num_workers)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, bert</span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTClassifier, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = bert.encoder</span><br><span class="line">        <span class="variable language_">self</span>.hidden = bert.hidden</span><br><span class="line">        <span class="variable language_">self</span>.output = nn.LazyLinear(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        tokens_X, segments_X, valid_lens_x = inputs</span><br><span class="line">        encoded_X = <span class="variable language_">self</span>.encoder(tokens_X, segments_X, valid_lens_x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.output(<span class="variable language_">self</span>.hidden(encoded_X[:, <span class="number">0</span>, :]))</span><br><span class="line">    </span><br><span class="line">net = BERTClassifier(bert)</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">1e-4</span>, <span class="number">5</span></span><br><span class="line">trainer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">net(<span class="built_in">next</span>(<span class="built_in">iter</span>(train_iter))[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch13</span>(<span class="params">net, train_iter, test_iter, loss, trainer, num_epochs,</span></span><br><span class="line"><span class="params">               devices=d2l.try_all_gpus(<span class="params"></span>)</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Train a model with multiple GPUs (defined in Chapter 13).&quot;&quot;&quot;</span></span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># Sum of training loss, sum of training accuracy, no. of examples,</span></span><br><span class="line">        <span class="comment"># no. of predictions</span></span><br><span class="line">        metric = d2l.Accumulator(<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            l, acc = d2l.train_batch_ch13(</span><br><span class="line">                net, features, labels, loss, trainer, devices)</span><br><span class="line">            metric.add(l, acc, labels.shape[<span class="number">0</span>], labels.numel())</span><br><span class="line">            timer.stop()</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">3</span>],</span><br><span class="line">                              <span class="literal">None</span>))</span><br><span class="line">        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存图像</span></span><br><span class="line">    plt.savefig(<span class="string">&#x27;training_progress.png&#x27;</span>)  <span class="comment"># 保存为图片文件</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>, train acc &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">3</span>]:<span class="number">.3</span>f&#125;</span>, test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec on &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span><br><span class="line"></span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.emptydust.com">风铃夜行</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.emptydust.com/2024/12/28/ml-server-wp/">https://www.emptydust.com/2024/12/28/ml-server-wp/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://www.emptydust.com" target="_blank">Fengling's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-share"><div class="social-share" data-image="https://www.helloimg.com/i/2024/12/28/676fa53a9c318.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/28/hello-world/" title="Hello World"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Hello World</div></div><div class="info-2"><div class="info-item-1">Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot;  More info: Writing Run server1$ hexo server  More info: Server Generate static files1$ hexo generate  More info: Generating Deploy to remote sites1$ hexo deploy  More info: Deployment </div></div></div></a><a class="pagination-related" href="/2024/10/08/xcpc-one-year-achieve/" title="算竞一周年祭-梦"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">算竞一周年祭-梦</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/28/transformer-replication/" title="transformer_replication"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-28</div><div class="info-item-2">transformer_replication</div></div><div class="info-2"><div class="info-item-1">复现transformerday1 配置环境&amp;下载数据集123conda create --name transformer python=3.8 -yconda activate transformerpip install torch torchvision torchaudio  1pip install datasets  如果没有安装上 1conda install -c conda-forge datasets  然后配置环境变量修改到国内镜像 12pip install -U huggingface_hubexport HF_ENDPOINT=https://hf-mirror.com  数据集：https://huggingface.co/datasets/wmt/wmt14 运行代码下载 12345678from datasets import load_datasetds = load_dataset(&quot;wmt/wmt14&quot;,...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://www.helloimg.com/i/2024/12/28/676fa53a9c318.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">风铃夜行</div><div class="author-info-description">魔法改变世界</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/EmptyDust"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/06/20250106/" title="20250106周报">20250106周报</a><time datetime="2025-01-06T14:05:12.000Z" title="发表于 2025-01-06 22:05:12">2025-01-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/28/transformer-replication/" title="transformer_replication">transformer_replication</a><time datetime="2024-12-28T09:15:46.000Z" title="发表于 2024-12-28 17:15:46">2024-12-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/28/hello-world/" title="Hello World">Hello World</a><time datetime="2024-12-28T08:07:57.000Z" title="发表于 2024-12-28 16:07:57">2024-12-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/28/ml-server-wp/" title="ml_server_wp">ml_server_wp</a><time datetime="2024-12-28T02:26:21.000Z" title="发表于 2024-12-28 10:26:21">2024-12-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/08/xcpc-one-year-achieve/" title="算竞一周年祭-梦">算竞一周年祭-梦</a><time datetime="2024-10-08T13:28:35.000Z" title="发表于 2024-10-08 21:28:35">2024-10-08</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 风铃夜行</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>