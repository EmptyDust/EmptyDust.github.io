<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Fengling&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="魔法改变世界">
<meta property="og:type" content="website">
<meta property="og:title" content="Fengling&#39;s Blog">
<meta property="og:url" content="https://www.emptydust.com/index.html">
<meta property="og:site_name" content="Fengling&#39;s Blog">
<meta property="og:description" content="魔法改变世界">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="风铃夜行">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Fengling&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.ico">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Fengling&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">[object Object]</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://www.emptydust.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-20250312" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/10/20250312/" class="article-date">
  <time datetime="2025-03-10T12:50:58.000Z" itemprop="datePublished">2025-03-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/10/20250312/">20250312周报</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="日报"><a href="#日报" class="headerlink" title="日报"></a>日报</h1><h2 id="20250306"><a href="#20250306" class="headerlink" title="20250306"></a>20250306</h2><p>阅读ChatDev代码，检索了解manus。</p>
<h2 id="20250307"><a href="#20250307" class="headerlink" title="20250307"></a>20250307</h2><p>突发奇想写了个简单的minecraft-codeforces agent，检索了解cursor。</p>
<h2 id="20250308"><a href="#20250308" class="headerlink" title="20250308"></a>20250308</h2><p>阅读论文《SOCIODOJO: BUILDING LIFELONG ANALYTICAL AGENTS WITH REAL-WORLD TEXT AND TIME SERIES》，学习有关agent环境搭建的内容。</p>
<h2 id="20250309"><a href="#20250309" class="headerlink" title="20250309"></a>20250309</h2><p>继续阅读，这论文怎么还自己造词呢，真莎士比亚啊。给校内老师帮忙出了点算法题。</p>
<h2 id="20250310"><a href="#20250310" class="headerlink" title="20250310"></a>20250310</h2><p>论文阅读收尾，写论文summary。qq群里出现了一个神级bot，我快要分不清了。</p>
<h2 id="20250311-20250312"><a href="#20250311-20250312" class="headerlink" title="20250311~20250312"></a>20250311~20250312</h2><p>阅读论文代码（代码质量令人遗憾），跟学院团委吵社团名字。</p>
<h1 id="Sociodojo-论文阅读"><a href="#Sociodojo-论文阅读" class="headerlink" title="Sociodojo 论文阅读"></a>Sociodojo 论文阅读</h1><p><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=s9z0HzWJJp">论文链接</a></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Sociodojo由四部分组成，信息来源、时间线、知识基础和工具箱。(1)<br>Task设计：基于一个简单逻辑，从可观测的 wt 推导出隐藏的 st（透过现象看本质）。考虑到消息传递需要时间，因此隐藏的 st 往往是过去的。(3.1.1)。<br>(index,metadata,messages) metadata是一个消息的summary，方便模型确定是否需要去看(3.1.1)<br>?(3.1.2)真没看懂，怎么感觉跟3.1.1没大区别，有点不懂。<br>sociodojo的信息来源主要是爬了一堆新闻媒体，金融数据，各种调查报告（顺便提了google数据的特殊处理，遇到相同的可以借鉴）；<br>讲了一些关于<strong>知识库</strong>的处理手段，以及（看起来很简陋的）工具箱的组成。(3.2.1)<br>世界运行器（很像瘟疫公司啊）和经纪人机制（限制一些不合理的交易）(3.2.2)<br>时间序列库由ChromaDB和Instructor-XL实现为两个向量数据库作为嵌入模型，利用该模型嵌入自然语言查询，然后计算其与数据库中存储的片段的嵌入的内积距离，并返回最佳候选。对于知识库，片段是第3.2.1节中讨论的分段文本;对于时间序列，代码片段是时间序列的代码及其描述，允许智能体找到所需时间序列的代码。<strong>这里想学习一下原理以及代码是怎么写的</strong>(4.1)<br>思路来源于<strong>零知识证明</strong>，我一直很感兴趣它的原理和基本知识，但一直没去了解。<br>分析师会做出猜测？–<strong>假设证明H&amp;P</strong>(4.2)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://www.emptydust.com/2025/03/10/20250312/" data-id="cm8fv3tnp0006kpqc20vq4gpt" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-20250305" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/03/05/20250305/" class="article-date">
  <time datetime="2025-03-05T12:16:58.000Z" itemprop="datePublished">2025-03-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/03/05/20250305/">20250305周报</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="日报"><a href="#日报" class="headerlink" title="日报"></a>日报</h1><h2 id="20250302"><a href="#20250302" class="headerlink" title="20250302"></a>20250302</h2><p>想了一上午到底自己是在做什么<br>下午按照计划打了一场算竞比赛<br>晚上跑cf网站的爬虫被反爬打趴</p>
<h2 id="20250303"><a href="#20250303" class="headerlink" title="20250303"></a>20250303</h2><p>爬虫依然被打趴。<br>向刘健师兄请教到底自己该怎么做，通过交流明确了很多基本概念，消除了我的很多疑惑并明确了需要做的事情————找到感兴趣的论文就去认真读并复现。</p>
<h2 id="20250304"><a href="#20250304" class="headerlink" title="20250304"></a>20250304</h2><p>想起老师所说的ChatDev，成功运行了它的代码，我尝试让他“生成一个五子棋程序”(ChatGPT3.5)，我认为这不算一个很难的要求，但发现好像效果不是很好。<br>注：原论文评估时就是用的GPT3.5。<br>同时阅读论文，大概读了小半。<br>另外群u给了个镜像站地址，爬虫站起来了，虽然已经没大用了。</p>
<h2 id="20250305"><a href="#20250305" class="headerlink" title="20250305"></a>20250305</h2><p>读完了论文，开始阅读代码，学习到了很多之前不会的python技巧，如使用包装器函数将对Message的访问重定向str类中。<br>回顾所学，写这篇周报。</p>
<h1 id="ChatDev-论文阅读"><a href="#ChatDev-论文阅读" class="headerlink" title="ChatDev 论文阅读"></a>ChatDev 论文阅读</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.07924">论文链接</a></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>仿照软件工程中分工模式，让模型分别扮演CEO(design), CTO(communicate with programmer), Programmer(coding), Reviewer(code review), Tester(testing)来开发一个程序，提高程序设计成功率。<br>其中，CEO负责理解用户需求并规划任务，CTO负责理解CEO给出的任务并与Programmer交互直到确认代码编写完成，接下来是Reviewer和Tester依次指出问题（两者区别是执行代码与否），由Programmer修复漏洞。</p>
<h2 id="学习到的知识以及感觉需要记录的东西"><a href="#学习到的知识以及感觉需要记录的东西" class="headerlink" title="学习到的知识以及感觉需要记录的东西"></a>学习到的知识以及感觉需要记录的东西</h2><h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><p>由于显然交流多次后聊天记录会很长，而目前模型无法处理巨长无比的聊天记录，因此作者整了一个长短期记忆，具体而言，短期记忆就是存双方近期聊天记录，长期记忆存储’solution’，我理解是只存了代码，关于这点需要继续阅读代码。</p>
<blockquote>
<p>By sharing only the solutions of each subtask<br>rather than the entire communication history, ChatDev minimizes the risk of being overwhelmed by<br>too much information, enhancing concentration on<br>each task and encouraging more targeted cooperation, while simultaneously facilitating cross-phase<br>context continuity.(3.1节最后一段)</p>
</blockquote>
<p>如果真的是这样的话是不是武断了点？</p>
<h3 id="执行者提问环节"><a href="#执行者提问环节" class="headerlink" title="执行者提问环节"></a>执行者提问环节</h3><p>图例：<br>A: assistant 执行者<br>I: instructor 构思者<br>-&gt; : 提要求&#x2F;提问<br>~&gt; : 回复&#x2F;给方案<br>正常来说一般是：<br>I -&gt; A, A ~&gt; I<br>作者加入了一个环节，要求 A 可以向 I 提问更多详细信息。<br>I -&gt; A, (A -&gt; I, I ~&gt; A)(这里可循环), A ~&gt; I<br>作者称这能够有效减少沟通性幻觉。</p>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>作者制作了一个专门的评估数据集。<br>评估标准分为完成性(能否独立完成)，可执行性，一致性(符不符合要求)，质量(这个是前面三个的一个加权结果)。作者给出了它们的计算方式。</p>
<h3 id="其他感受"><a href="#其他感受" class="headerlink" title="其他感受"></a>其他感受</h3><p>这篇文章的作者英文水平比之前看的好的多，用词仿佛莎士比亚，导致我到处不认识单词。直观感觉这篇文章中所提到的东西都称不上难，这是一种错觉吗（？，或许是文章写得好容易理解。</p>
<p>之前在组会上红温(字面义，我的语言功能因各种原因紧张而丧失时会整个脑袋升温，表现为面部耳部一秒变红肿大)时想表达的东西，大致就是给Programmer增加一个Planner以及一个监督者，类似于这篇文章中CEO和CTO的定位。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://www.emptydust.com/2025/03/05/20250305/" data-id="cm8fv3tno0005kpqcdpvohzvl" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-20250219" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/02/19/20250219/" class="article-date">
  <time datetime="2025-02-19T14:15:12.000Z" itemprop="datePublished">2025-02-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/02/19/20250219/">20250219周报</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="周报-20250213-20250219"><a href="#周报-20250213-20250219" class="headerlink" title="周报 20250213~20250219"></a>周报 20250213~20250219</h1><h2 id="论文阅读及复现"><a href="#论文阅读及复现" class="headerlink" title="论文阅读及复现"></a>论文阅读及复现</h2><h3 id="正在阅读-Deepseek-LLM"><a href="#正在阅读-Deepseek-LLM" class="headerlink" title="正在阅读 Deepseek LLM"></a>正在阅读 Deepseek LLM</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.02954">DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</a></p>
<h4 id="学习了-LlaMA-架构"><a href="#学习了-LlaMA-架构" class="headerlink" title="学习了 LlaMA 架构"></a>学习了 LlaMA 架构</h4><p>pre-Norm 把归一层放前面(我在复现Transformers架构时好像也这么做了) </p>
<p>RMSNorm 一种更高效的归一层<br>与传统的Layer Norm相比，RMSNorm计算起来更加高效，因为它不需要对输入进行减去均值的操作，从而减少了计算量和内存占用。</p>
<blockquote>
<p>给定一个输入向量(x)，RMSNorm的输出可以表示为：<br>[ y &#x3D; \frac{x}{\sqrt{\mathbb{E}[x^2] + \epsilon}} \cdot \gamma ]</p>
<p>这里：</p>
<ul>
<li>( \mathbb{E}[x^2] ) 表示输入向量(x)各元素平方的平均值。</li>
<li>( \epsilon ) 是一个小的常数，用于防止分母为零的情况，保证数值稳定性。</li>
<li>( \gamma ) 是一个可学习的参数，用于缩放归一化后的结果。</li>
</ul>
</blockquote>
<p>SwiGLU，一种具有门控机制的激活函数，增强模型非线性表达能力。</p>
<blockquote>
<p>SwiGLU(x) &#x3D; (Swish(xW) ⊙ xV) * b<br>Swish(x) &#x3D; x * sigmoid(βx)</p>
</blockquote>
<p>Rotary Embedding 将位置信息编码为旋转矩阵，作用于查询和键向量。<br>借助了复数的思想，通过绝对位置编码的方式实现了相对位置编码的效果</p>
<p>Grouped-Query Attention 将查询头分组，每组共享相同的键和值头。<br>可以在比较大的参数量时使用</p>
<h4 id="超参选取实验"><a href="#超参选取实验" class="headerlink" title="超参选取实验"></a>超参选取实验</h4><blockquote>
<p>result:<br>$ 𝜂_{opt} &#x3D; 0.3118 \times 𝐶^{−0.1250} $<br>$ 𝐵_{opt} &#x3D; 0.2920 \times 𝐶^{0.3271} $</p>
</blockquote>
<blockquote>
<p>好久不见，复杂度（这里计算了参数量）</p>
</blockquote>
<blockquote>
<p>在增加计算预算时，扩大模型规模比增加数据规模更有优势</p>
</blockquote>
<h3 id="MP5论文复现进度"><a href="#MP5论文复现进度" class="headerlink" title="MP5论文复现进度"></a>MP5论文复现进度</h3><p>总结：由于对虚拟机和docker等基本知识不够了解，浪费大量时间，因此几乎没有（</p>
<ol>
<li>搞定了openai的api</li>
<li>minecraft agent的辅助库 minedojo 基于ubunto或macOS 尝试使用虚拟机搭建但不是很成功（虚拟机低手）</li>
<li>目前遇到不知道如何在虚拟机环境搭建代理的问题-平时使用的代理程序没有linux版本，正在寻找解决方案</li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>背单词</li>
<li>超前课堂进度自学操作系统中</li>
<li>在自己的虚拟机中尝试使用 rm &#x2F;* 删除当前文件夹并成功重装（还好是空的虚拟机</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://www.emptydust.com/2025/02/19/20250219/" data-id="cm8fv3tno0004kpqcd6gr1m0m" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-20250120" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/01/20/20250120/" class="article-date">
  <time datetime="2025-01-20T14:15:12.000Z" itemprop="datePublished">2025-01-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/01/20/20250120/">20250113周报</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="周报-2025014-20250120"><a href="#周报-2025014-20250120" class="headerlink" title="周报 2025014~20250120"></a>周报 2025014~20250120</h1><h2 id="transformers复现"><a href="#transformers复现" class="headerlink" title="transformers复现"></a>transformers复现</h2><p>d_model &#x3D; 512<br>n_layers &#x3D; 6<br>n_heads &#x3D; 8<br>ff_hidden_dim &#x3D; 2048</p>
<p>均使用贪心搜索。 原论文使用 beam&#x3D;4, alpha&#x3D;0.6 的束搜索<br>原论文小模型bleu4分数为 28.4</p>
<table>
<thead>
<tr>
<th>lr</th>
<th>batch_size</th>
<th>epoch_cnt</th>
<th>final_loss</th>
<th>bleu4 score</th>
<th>description</th>
</tr>
</thead>
<tbody><tr>
<td>0.000005</td>
<td>64</td>
<td>2</td>
<td>3.23</td>
<td>6</td>
<td></td>
</tr>
<tr>
<td>0.00005</td>
<td>64</td>
<td>2</td>
<td>1.95</td>
<td>23.7</td>
<td></td>
</tr>
<tr>
<td>0.00003</td>
<td>64</td>
<td>3</td>
<td>1.93</td>
<td>24.0</td>
<td></td>
</tr>
<tr>
<td>0.00001</td>
<td>64</td>
<td>4</td>
<td>2.08</td>
<td>21.5</td>
<td></td>
</tr>
<tr>
<td>0.0005</td>
<td>?</td>
<td>?</td>
<td>?</td>
<td>28.4</td>
<td>原论文</td>
</tr>
</tbody></table>
<h2 id="代码能力提升计划"><a href="#代码能力提升计划" class="headerlink" title="代码能力提升计划"></a>代码能力提升计划</h2><h3 id="“良好的代码能力”究竟指什么"><a href="#“良好的代码能力”究竟指什么" class="headerlink" title="“良好的代码能力”究竟指什么"></a>“良好的代码能力”究竟指什么</h3><p>不太懂，感觉在语境下指的是类似于项目能力的样子，能自行完成环境搭建和代码编写用来把积木搭出来。</p>
<p>环境搭建需要较强的信息检索能力，代码编写主要要求能理解代码运行逻辑（然后其实就可以写了吧……）。</p>
<h3 id="可能的一些建议"><a href="#可能的一些建议" class="headerlink" title="可能的一些建议"></a>可能的一些建议</h3><blockquote>
<p>针对有c以及python基础，面向对象语言基础的同学，如群里的“走肖木同”，我觉得或许他甚至已经可以直接上手。<br>他可能可以的改进方向是学习简单的算法竞赛知识，比如前缀和、差分等，这些可以让他不至于搞出非常愚蠢的代码导致时间复杂度爆炸（。<br>我认为动态规划、滑动窗口、图论对深度学习的学习有辅助作用，可以适当接触。</p>
<p>针对没有c及python基础的同学，建议先搞懂基础语法，我没有具体的建议，因为我学这些时已经有在minecraft中敲了4年指令经验，这导致我完全速通了它们。<br>要记得搞懂递归等有些难度的东西，理解并熟练使用面向对象的思想。</p>
<p>我认为信息检索能力是最重要的，但我不了解这个如何训练QwQ。</p>
</blockquote>
<h2 id="组会上所说的方向"><a href="#组会上所说的方向" class="headerlink" title="组会上所说的方向"></a>组会上所说的方向</h2><blockquote>
<p>还是有点没太搞懂到底这是啥来着2333<br>目前还在发散性思考ing</p>
</blockquote>
<p>以下是一些发散性思考，求批评指正QwQ:</p>
<ol>
<li>在人工智能点网页这个故事背景下，回退的情况我认为原因是网页内容与预期不同，是我们的模型在点击后才发现的情况导致它意识到这一问题而选择回退。因此，存在回退的基本条件是我们在新的状态中能获得更多的信息。</li>
<li>很难想象它是如何用在AI编程中，或许是调用一下编译器告诉一下它存在编译错误或者是解析结果反馈代码模块？</li>
<li>它们的关系很像图，如果是稀疏图的话或许可以用图论优化些什么？（</li>
</ol>
<p>有以下一些问题QwQ:</p>
<ol>
<li>去哪里找资料&#x2F;论文比较好，如何挑选有东西的论文</li>
</ol>
<h2 id="下周计划"><a href="#下周计划" class="headerlink" title="下周计划"></a>下周计划</h2><p>启动那个搞不懂到底叫啥的方向（<br>找些论文了解情况。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://www.emptydust.com/2025/01/20/20250120/" data-id="cm8fv3tnn0003kpqc53in3otm" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-20250113" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/01/11/20250113/" class="article-date">
  <time datetime="2025-01-11T06:15:12.000Z" itemprop="datePublished">2025-01-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/01/11/20250113/">20250113周报</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="周报-20250107-20250113"><a href="#周报-20250107-20250113" class="headerlink" title="周报 20250107~20250113"></a>周报 20250107~20250113</h1><h2 id="还在搞transformer"><a href="#还在搞transformer" class="headerlink" title="还在搞transformer"></a>还在搞transformer</h2><h3 id="使用已有的tokenizer进行数据预处理"><a href="#使用已有的tokenizer进行数据预处理" class="headerlink" title="使用已有的tokenizer进行数据预处理"></a>使用已有的tokenizer进行数据预处理</h3><h3 id="追踪向量形状"><a href="#追踪向量形状" class="headerlink" title="追踪向量形状"></a>追踪向量形状</h3><p>多打注释。</p>
<h3 id="multiheadattention-的输入与预期不同"><a href="#multiheadattention-的输入与预期不同" class="headerlink" title="multiheadattention 的输入与预期不同"></a>multiheadattention 的输入与预期不同</h3><p>[seq_len, batch_size, d_model]</p>
<h3 id="更改运算位置到GPU"><a href="#更改运算位置到GPU" class="headerlink" title="更改运算位置到GPU"></a>更改运算位置到GPU</h3><p>记得多打 .to(device)</p>
<h3 id="有些mask是不需要的"><a href="#有些mask是不需要的" class="headerlink" title="有些mask是不需要的"></a>有些mask是不需要的</h3><p>比如mem_mask</p>
<h3 id="不合理的tgt设置导致出现模型偷看答案"><a href="#不合理的tgt设置导致出现模型偷看答案" class="headerlink" title="不合理的tgt设置导致出现模型偷看答案"></a>不合理的tgt设置导致出现模型偷看答案</h3><p>以及一开始没搞懂什么叫做 shift right</p>
<h3 id="出现nan，尚未找到原因"><a href="#出现nan，尚未找到原因" class="headerlink" title="出现nan，尚未找到原因"></a>出现nan，尚未找到原因</h3><p>nan的原因是因为把attn_mask 的True和False写反了<br>然后就出现模型极其容易梯度爆炸的问题，明天继续调……<br>现在它能正常运行7个batch……然后梯度爆炸</p>
<h2 id="下周计划"><a href="#下周计划" class="headerlink" title="下周计划"></a>下周计划</h2><p>在保持心态不爆炸的情况下搞定这个玩意</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://www.emptydust.com/2025/01/11/20250113/" data-id="cm8fv3tnm0002kpqchnim69az" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-20250106" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2025/01/06/20250106/" class="article-date">
  <time datetime="2025-01-06T14:05:12.000Z" itemprop="datePublished">2025-01-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2025/01/06/20250106/">20250106周报</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="周报-20241231-20250106"><a href="#周报-20241231-20250106" class="headerlink" title="周报 20241231~20250106"></a>周报 20241231~20250106</h1><blockquote>
<p>啊？怎么一周过去了<br>啊？怎么一年过去了</p>
</blockquote>
<h2 id="我做了啥"><a href="#我做了啥" class="headerlink" title="我做了啥"></a>我做了啥</h2><h3 id="DL学习方面-20241231-20250103"><a href="#DL学习方面-20241231-20250103" class="headerlink" title="DL学习方面 20241231~20250103"></a>DL学习方面 20241231~20250103</h3><p>目前在复现transformer阶段，很遗憾这周并没有很多时间搞这个。目前能够理解它的原理，但是代码实在不太（极其不）熟练，可能还需要一些时间调试并确保搞懂代码。主要困难来源于搞不懂torch中xx函数xx类构造方法参数之类的问题，以及一些愚蠢的手误。</p>
<h3 id="回家跨了个年-20241231-20250101"><a href="#回家跨了个年-20241231-20250101" class="headerlink" title="回家跨了个年 20241231~20250101"></a>回家跨了个年 20241231~20250101</h3><h3 id="课程设计-20250102-20250107"><a href="#课程设计-20250102-20250107" class="headerlink" title="课程设计 20250102~20250107"></a>课程设计 20250102~20250107</h3><p>这周是数据结构课程设计时间，需要挺多时间用来写代码，本来是不需要很多的，但是我找了个好玩的题目，并且我的老师们热衷于看可视化，所以虽然算法部分只花了一天时间看论文，半天时间写代码，但花费了极其大量的时间学习可视化。过程中掌握了基础的networkx和manim两个可视化库的应用，制作了简单的图可视化以及3b1b同款视频来演示。<br>感觉有点但不是非常浪费时间，一是这门课的分数大概还不错的，二是3b1b的视频风格真的很帅。</p>
<h2 id="下周计划"><a href="#下周计划" class="headerlink" title="下周计划"></a>下周计划</h2><ol>
<li>明天（20240107）要被大巴拉出去认识实习并写个报告。</li>
<li>搞定transformer复现</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://www.emptydust.com/2025/01/06/20250106/" data-id="cm8fv3tnj0001kpqcht9q13tx" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-transformer-replication" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/12/28/transformer-replication/" class="article-date">
  <time datetime="2024-12-28T09:15:46.000Z" itemprop="datePublished">2024-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/12/28/transformer-replication/">transformer_replication</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="复现transformer"><a href="#复现transformer" class="headerlink" title="复现transformer"></a>复现transformer</h1><h2 id="day1-配置环境-下载数据集"><a href="#day1-配置环境-下载数据集" class="headerlink" title="day1 配置环境&amp;下载数据集"></a>day1 配置环境&amp;下载数据集</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create --name transformer python=3.8 -y</span><br><span class="line">conda activate transformer</span><br><span class="line">pip install torch torchvision torchaudio</span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install datasets</span><br></pre></td></tr></table></figure>

<p>如果没有安装上</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge datasets</span><br></pre></td></tr></table></figure>

<p>然后配置环境变量修改到国内镜像</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br><span class="line"><span class="built_in">export</span> HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure>

<p>数据集：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/wmt/wmt14">https://huggingface.co/datasets/wmt/wmt14</a></p>
<p>运行代码下载</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds = load_dataset(<span class="string">&quot;wmt/wmt14&quot;</span>, <span class="string">&quot;de-en&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ds)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ds[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>数据集下载在了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/.cache/huggingface/datasets/wmt___wmt14/</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/CoolBoySilverBullet/article/details/123365452">本地配置</a></p>
<h2 id="day2"><a href="#day2" class="headerlink" title="day2"></a>day2</h2><h1 id="inference"><a href="#inference" class="headerlink" title="inference"></a>inference</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40959890/article/details/140319652">hf-mirror （huggingface 的国内镜像）</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://www.emptydust.com/2024/12/28/transformer-replication/" data-id="cm8fv3tnu000ckpqc17zq3wml" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/12/28/hello-world/" class="article-date">
  <time datetime="2024-12-28T08:07:57.000Z" itemprop="datePublished">2024-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/12/28/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://www.emptydust.com/2024/12/28/hello-world/" data-id="cm8fv3tnt000akpqc5i3896o0" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-ml-server-wp" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/12/28/ml-server-wp/" class="article-date">
  <time datetime="2024-12-28T02:26:21.000Z" itemprop="datePublished">2024-12-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/catag-test/">catag_test</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/12/28/ml-server-wp/">ml_server_wp</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>使用vscode的remote-ssh拓展连接服务器</p>
<p>我不是很确定我是如何搞定这个的，但可能是这样:<br>使用shell将conda加入path使其全局可用</p>
<p>创建名为code的目录，然后在vsc中打开它，就可以在vsc的终端中使用它了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir code</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">init.sh</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">&gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">!! Contents within this block are managed by <span class="string">&#x27;conda init&#x27;</span> !!</span></span><br><span class="line">__conda_setup=&quot;$(&#x27;/usr/local/miniconda3/bin/conda&#x27; &#x27;shell.bash&#x27; &#x27;hook&#x27; 2&gt; /dev/null)&quot;</span><br><span class="line">if [ $? -eq 0 ]; then</span><br><span class="line">    eval &quot;$__conda_setup&quot;</span><br><span class="line">else</span><br><span class="line">    if [ -f &quot;/usr/local/miniconda3/etc/profile.d/conda.sh&quot; ]; then</span><br><span class="line">        . &quot;/usr/local/miniconda3/etc/profile.d/conda.sh&quot;</span><br><span class="line">    else</span><br><span class="line">        export PATH=&quot;/usr/local/miniconda3/bin:$PATH&quot;</span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line">unset __conda_setup</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">&lt;&lt;&lt; <span class="string">conda initialize &lt;&lt;</span></span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh init.sh</span><br></pre></td></tr></table></figure>

<p>以<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html">某一章节</a>的代码作为例子。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda create --name d2l python=3.9 -y</span><br><span class="line">conda activate d2l</span><br><span class="line">pip install torch==2.0.0 torchvision==0.15.1</span><br><span class="line">pip install d2l==1.0.3</span><br></pre></td></tr></table></figure>

<p>不建议使用jupyter，而是以输出图片到相同文件夹下代替<br>因此相比原文重写了train_ch13函数</p>
<p>运行成功的代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;bert.base&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;bert.base.torch.zip&#x27;</span>,</span><br><span class="line">                             <span class="string">&#x27;225d66f04cae318b841a13d32af3acc165f253ac&#x27;</span>)</span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;bert.small&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;bert.small.torch.zip&#x27;</span>,</span><br><span class="line">                              <span class="string">&#x27;c72329e68a732bef0452e4b96a1c341c8910f81f&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_pretrained_model</span>(<span class="params">pretrained_model, num_hiddens, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                          num_heads, num_blks, dropout, max_len, devices</span>):</span><br><span class="line">    data_dir = d2l.download_extract(pretrained_model)</span><br><span class="line">    <span class="comment"># Define an empty vocabulary to load the predefined vocabulary</span></span><br><span class="line">    vocab = d2l.Vocab()</span><br><span class="line">    vocab.idx_to_token = json.load(<span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;vocab.json&#x27;</span>)))</span><br><span class="line">    vocab.token_to_idx = &#123;token: idx <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">        vocab.idx_to_token)&#125;</span><br><span class="line">    bert = d2l.BERTModel(</span><br><span class="line">        <span class="built_in">len</span>(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=<span class="number">4</span>,</span><br><span class="line">        num_blks=<span class="number">2</span>, dropout=<span class="number">0.2</span>, max_len=max_len)</span><br><span class="line">    <span class="comment"># Load pretrained BERT parameters</span></span><br><span class="line">    bert.load_state_dict(torch.load(os.path.join(data_dir,</span><br><span class="line">                                                 <span class="string">&#x27;pretrained.params&#x27;</span>)))</span><br><span class="line">    <span class="keyword">return</span> bert, vocab</span><br><span class="line"></span><br><span class="line">devices = d2l.try_all_gpus()</span><br><span class="line">bert, vocab = load_pretrained_model(</span><br><span class="line">    <span class="string">&#x27;bert.small&#x27;</span>, num_hiddens=<span class="number">256</span>, ffn_num_hiddens=<span class="number">512</span>, num_heads=<span class="number">4</span>,</span><br><span class="line">    num_blks=<span class="number">2</span>, dropout=<span class="number">0.1</span>, max_len=<span class="number">512</span>, devices=devices)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SNLIBERTDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, max_len, vocab=<span class="literal">None</span></span>):</span><br><span class="line">        all_premise_hypothesis_tokens = [[</span><br><span class="line">            p_tokens, h_tokens] <span class="keyword">for</span> p_tokens, h_tokens <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">            *[d2l.tokenize([s.lower() <span class="keyword">for</span> s <span class="keyword">in</span> sentences])</span><br><span class="line">              <span class="keyword">for</span> sentences <span class="keyword">in</span> dataset[:<span class="number">2</span>]])]</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.labels = torch.tensor(dataset[<span class="number">2</span>])</span><br><span class="line">        <span class="variable language_">self</span>.vocab = vocab</span><br><span class="line">        <span class="variable language_">self</span>.max_len = max_len</span><br><span class="line">        (<span class="variable language_">self</span>.all_token_ids, <span class="variable language_">self</span>.all_segments,</span><br><span class="line">         <span class="variable language_">self</span>.valid_lens) = <span class="variable language_">self</span>._preprocess(all_premise_hypothesis_tokens)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.all_token_ids)) + <span class="string">&#x27; examples&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_preprocess</span>(<span class="params">self, all_premise_hypothesis_tokens</span>):</span><br><span class="line">        pool = multiprocessing.Pool(<span class="number">4</span>)  <span class="comment"># Use 4 worker processes</span></span><br><span class="line">        out = pool.<span class="built_in">map</span>(<span class="variable language_">self</span>._mp_worker, all_premise_hypothesis_tokens)</span><br><span class="line">        all_token_ids = [</span><br><span class="line">            token_ids <span class="keyword">for</span> token_ids, segments, valid_len <span class="keyword">in</span> out]</span><br><span class="line">        all_segments = [segments <span class="keyword">for</span> token_ids, segments, valid_len <span class="keyword">in</span> out]</span><br><span class="line">        valid_lens = [valid_len <span class="keyword">for</span> token_ids, segments, valid_len <span class="keyword">in</span> out]</span><br><span class="line">        <span class="keyword">return</span> (torch.tensor(all_token_ids, dtype=torch.long),</span><br><span class="line">                torch.tensor(all_segments, dtype=torch.long),</span><br><span class="line">                torch.tensor(valid_lens))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_mp_worker</span>(<span class="params">self, premise_hypothesis_tokens</span>):</span><br><span class="line">        p_tokens, h_tokens = premise_hypothesis_tokens</span><br><span class="line">        <span class="variable language_">self</span>._truncate_pair_of_tokens(p_tokens, h_tokens)</span><br><span class="line">        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)</span><br><span class="line">        token_ids = <span class="variable language_">self</span>.vocab[tokens] + [<span class="variable language_">self</span>.vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] \</span><br><span class="line">                             * (<span class="variable language_">self</span>.max_len - <span class="built_in">len</span>(tokens))</span><br><span class="line">        segments = segments + [<span class="number">0</span>] * (<span class="variable language_">self</span>.max_len - <span class="built_in">len</span>(segments))</span><br><span class="line">        valid_len = <span class="built_in">len</span>(tokens)</span><br><span class="line">        <span class="keyword">return</span> token_ids, segments, valid_len</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_truncate_pair_of_tokens</span>(<span class="params">self, p_tokens, h_tokens</span>):</span><br><span class="line">        <span class="comment"># Reserve slots for &#x27;&lt;CLS&gt;&#x27;, &#x27;&lt;SEP&gt;&#x27;, and &#x27;&lt;SEP&gt;&#x27; tokens for the BERT</span></span><br><span class="line">        <span class="comment"># input</span></span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(p_tokens) + <span class="built_in">len</span>(h_tokens) &gt; <span class="variable language_">self</span>.max_len - <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(p_tokens) &gt; <span class="built_in">len</span>(h_tokens):</span><br><span class="line">                p_tokens.pop()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                h_tokens.pop()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="variable language_">self</span>.all_token_ids[idx], <span class="variable language_">self</span>.all_segments[idx],</span><br><span class="line">                <span class="variable language_">self</span>.valid_lens[idx]), <span class="variable language_">self</span>.labels[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.all_token_ids)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reduce `batch_size` if there is an out of memory error. In the original BERT</span></span><br><span class="line"><span class="comment"># model, `max_len` = 512</span></span><br><span class="line">batch_size, max_len, num_workers = <span class="number">512</span>, <span class="number">128</span>, d2l.get_dataloader_workers()</span><br><span class="line">data_dir = d2l.download_extract(<span class="string">&#x27;SNLI&#x27;</span>)</span><br><span class="line">train_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class="literal">True</span>), max_len, vocab)</span><br><span class="line">test_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class="literal">False</span>), max_len, vocab)</span><br><span class="line">train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                   num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(test_set, batch_size,</span><br><span class="line">                                  num_workers=num_workers)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, bert</span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTClassifier, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = bert.encoder</span><br><span class="line">        <span class="variable language_">self</span>.hidden = bert.hidden</span><br><span class="line">        <span class="variable language_">self</span>.output = nn.LazyLinear(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        tokens_X, segments_X, valid_lens_x = inputs</span><br><span class="line">        encoded_X = <span class="variable language_">self</span>.encoder(tokens_X, segments_X, valid_lens_x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.output(<span class="variable language_">self</span>.hidden(encoded_X[:, <span class="number">0</span>, :]))</span><br><span class="line">    </span><br><span class="line">net = BERTClassifier(bert)</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">1e-4</span>, <span class="number">5</span></span><br><span class="line">trainer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">net(<span class="built_in">next</span>(<span class="built_in">iter</span>(train_iter))[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch13</span>(<span class="params">net, train_iter, test_iter, loss, trainer, num_epochs,</span></span><br><span class="line"><span class="params">               devices=d2l.try_all_gpus(<span class="params"></span>)</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Train a model with multiple GPUs (defined in Chapter 13).&quot;&quot;&quot;</span></span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># Sum of training loss, sum of training accuracy, no. of examples,</span></span><br><span class="line">        <span class="comment"># no. of predictions</span></span><br><span class="line">        metric = d2l.Accumulator(<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            l, acc = d2l.train_batch_ch13(</span><br><span class="line">                net, features, labels, loss, trainer, devices)</span><br><span class="line">            metric.add(l, acc, labels.shape[<span class="number">0</span>], labels.numel())</span><br><span class="line">            timer.stop()</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">3</span>],</span><br><span class="line">                              <span class="literal">None</span>))</span><br><span class="line">        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存图像</span></span><br><span class="line">    plt.savefig(<span class="string">&#x27;training_progress.png&#x27;</span>)  <span class="comment"># 保存为图片文件</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>, train acc &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">3</span>]:<span class="number">.3</span>f&#125;</span>, test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec on &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://www.emptydust.com/2024/12/28/ml-server-wp/" data-id="cm8fv3tnv000fkpqceb11g2c6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-xcpc-one-year-achieve" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/10/08/xcpc-one-year-achieve/" class="article-date">
  <time datetime="2024-10-08T13:28:35.000Z" itemprop="datePublished">2024-10-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/10/08/xcpc-one-year-achieve/">算竞一周年祭-梦</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>当我打开手机准备写下这篇一周年祭时，我才发现已经十月八日了，距离我注册洛谷第一个账号已经过去了一年零八个小时。<br>嗯，我本以为今天是十月七日，还有一天留给我仔细打磨一下这篇文章，会不会因为我高达89分的高考语文成绩使它拥有过多的语病和逻辑不顺。当然，已经晚了。<br>我很早，大概是昨天，哦不，前天，或者几个月前，又或者是第一次认真对待算法竞赛时，就准备写下这样一篇小文，正式祭奠一下我那满是遗憾的日子。事实上，我没有机会写很多次这样的小文章，在我第一次知道算竞时，我已经18岁，大一了。<br>我正坐在上海电力大学第一食堂三楼的观景椅上，穿着短袖短裤，有些冷，但还能接受，毕竟这里的景色，已经是这个学校中最漂亮的了。我是怎么来到这里的呢？我想我不太清楚，又很明白。我一直活在童年，一个梦幻的童年，在那里我是一个数学天才、围棋天才，是家人寄予厚望的后生，是梦想成为一名数学家的天真小朋友。我在稀里糊涂的日子里度过了人生中最重要的日子，它们即将又已然决定了我人生的主干道。<br>我的中学生活是混沌的，我能够轻而易举地想到众多我所做的愚蠢的事情，其中绝大部分直到高二时某个早晨才终于被我发现，我就像一个醉汉，在梦幻中浪费了我的中学时光。而对于大多数无论成败的oier来说，他们则在爱好和梦想中度过这段最美好的时光。<br>我可能一度上不了大学，或许是大部分时间中，我都没有能力考上哪怕民办本科。但我从未这样想过，从小到大，我的周围充斥着清北复交的传说。我是无法活着听闻我上不了大学这件事的，我想。所以在高三时候，我终于认真对待了一次学习，靠着运气进入了上海电力大学。我的父母和周边人都认为这是一个相对很好的终点，他们觉得，这里背靠电网，想必能轻易获取一个稳定又舒适的生活。但我从未这样想过，我从未想过任何未来，我只是想活下去。<br>我的人生在很长一段时间里毫无意义，它空有十八年的长度，厚度却寥寥无几。我的存在对世界毫无作用，而世界只给我带来痛苦，我们相互都不太需要对方。这些年里真正称得上爱好的事情只有数学、围棋、听音乐。我没有什么娱乐生活，除去那些麻痹自我消磨时间的游戏外，每年去商场的次数可能不超过二十次，即便去商场也只是找个心仪且便宜（但其实我觉得还是很贵）的饭店吃饭。我是个假二次元，我看过一些老番，从未给二次元花过一分钱。在我所在的世界里，二次元太美好了，谷子太贵了。<br>不到一个月前，第一次有女孩子与我商讨出门玩什么时，我才终于发现，与身边很多人相比，我的过去经历如此贫困，我这样的人，是梦不见美少女的，梦不见与朋友们扫街团建，梦不见白月光和大悦城的辉煌的。显而易见的是，最终没有成功一起玩，甚至没有离开家门一次，也大概率不会有下一次讨论了。以后的我，大概还会如以前一般，“重视精神食粮”吧，哈哈。<br>对我来说，算法竞赛是一个好的精神鸦片，我疑似在这方面有些天赋，一些微不足道的成就给我带来了一些对我来说很有用的称赞。但好像我已经在错误的时间和错误的地点到达这里了，如果我在小学或初中时遇到算竞，我大概至少有一个积极的人生态度，小概有一个拼搏出的美好的人生；如果我在一个对的学校遇到算竞，我或许不需要每次遇到难题都自己一个人反复看四五个小时题解和别人的代码，或许不需要为赛站和经费发愁和交际。多么可笑啊，我们将浪费掉7个icpc名额中的5个，剩下的2个估计也得自费。<br>一年以前的我如何也想不到今天的我已经达到了codeforces全球1000名，名字已经变橙了，听说这好像不是一个很容易达到的成就。传说中最应当了解我的家人也没有想到，甚至他们中有很多都正在或曾经在各个学校中教计算机甚至算法，他们曾觉得我笨，我身体不好，所以不适合打竞赛。真是滑稽又可笑的命运啊。<br>高二时那个早晨我知道我迟到了五年，如今我知道我已经迟到了很久很久，或许是八年，或许是十年，在这段时间中太多的本当如此没有发生。或许，我至今仍然在迟到。<br>我走在回寝室的路上，路上很冷，临港很空旷，风很大，空气很冷，一个人。我想，大三时候估计要准备考研或者换方向搞ai又或者实习了，毕竟算法竞赛不能当饭吃。<br>看起来这是一篇与过去一年关联不大的流水账，但没关系，应该还会有下一篇，所以就这样结束吧。<br>2024.10.8 21时 于上海电力大学 风铃</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://www.emptydust.com/2024/10/08/xcpc-one-year-achieve/" data-id="cm8fv3tnv000gkpqc6x0o1sn1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%9A%8F%E7%AC%94/" rel="tag">随笔</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">下一页 &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/catag-test/">catag_test</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/homework/" rel="tag">homework</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/icpc/" rel="tag">icpc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E7%AC%94/" rel="tag">随笔</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/homework/" style="font-size: 10px;">homework</a> <a href="/tags/icpc/" style="font-size: 20px;">icpc</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E9%9A%8F%E7%AC%94/" style="font-size: 10px;">随笔</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">三月 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">二月 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">一月 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">十二月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">十月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/03/10/20250312/">20250312周报</a>
          </li>
        
          <li>
            <a href="/2025/03/05/20250305/">20250305周报</a>
          </li>
        
          <li>
            <a href="/2025/02/19/20250219/">20250219周报</a>
          </li>
        
          <li>
            <a href="/2025/01/20/20250120/">20250113周报</a>
          </li>
        
          <li>
            <a href="/2025/01/11/20250113/">20250113周报</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2025 风铃夜行<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>