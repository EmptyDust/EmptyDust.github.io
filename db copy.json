{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"acad91ace80b80295b11a9b7ad4c29a2dcfdd8fb","modified":1735352054546},{"_id":"source/_posts/ml-server-wp.md","hash":"a9d5cc56de5b2a1bc02414cc17808b570a728a00","modified":1735352781927},{"_id":"source/index/index.md","hash":"6fe73a5e2314f2ce69314724f438062867e6bce3","modified":1735355377996},{"_id":"public/index/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1735355423096},{"_id":"public/2024/12/28/ml-server-wp/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1735355423096},{"_id":"public/2024/12/28/hello-world/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1735355423096},{"_id":"public/archives/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1735355423096},{"_id":"public/archives/2024/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1735355423096},{"_id":"public/archives/2024/12/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1735355423096},{"_id":"public/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1735355423096}],"Category":[],"Data":[],"Page":[{"title":"index","date":"2024-12-28T03:09:37.000Z","_content":"","source":"index/index.md","raw":"---\ntitle: index\ndate: 2024-12-28 11:09:37\n---\n","updated":"2024-12-28T03:09:37.996Z","path":"index/index.html","comments":1,"layout":"page","_id":"cm57ltacu00009wnq9zkucgo6","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2024-12-28T02:14:14.546Z","updated":"2024-12-28T02:14:14.546Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cm57ltacx00019wnq24zhe90v","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"ml_server_wp","date":"2024-12-28T02:26:21.000Z","_content":"\n\n使用vscode的remote-ssh拓展连接服务器\n\n我不是很确定我是如何搞定这个的，但可能是这样:\n使用shell将conda加入path使其全局可用\n\n创建名为code的目录，然后在vsc中打开它，就可以在vsc的终端中使用它了。\n```shell\nmkdir code\n```\n\n```shell\n# init.sh\n# >>> conda initialize >>>\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('/usr/local/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"/usr/local/miniconda3/etc/profile.d/conda.sh\" ]; then\n        . \"/usr/local/miniconda3/etc/profile.d/conda.sh\"\n    else\n        export PATH=\"/usr/local/miniconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# <<< conda initialize <<\n```\n\n```shell\nsh init.sh\n```\n\n以[某一章节](https://d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html)的代码作为例子。\n\n```shell\nconda create --name d2l python=3.9 -y\nconda activate d2l\npip install torch==2.0.0 torchvision==0.15.1\npip install d2l==1.0.3\n```\n\n不建议使用jupyter，而是以输出图片到相同文件夹下代替\n因此相比原文重写了train_ch13函数\n\n运行成功的代码：\n\n```py\nimport json\nimport multiprocessing\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nd2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n                             '225d66f04cae318b841a13d32af3acc165f253ac')\nd2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n                              'c72329e68a732bef0452e4b96a1c341c8910f81f')\n\ndef load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n                          num_heads, num_blks, dropout, max_len, devices):\n    data_dir = d2l.download_extract(pretrained_model)\n    # Define an empty vocabulary to load the predefined vocabulary\n    vocab = d2l.Vocab()\n    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n        vocab.idx_to_token)}\n    bert = d2l.BERTModel(\n        len(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=4,\n        num_blks=2, dropout=0.2, max_len=max_len)\n    # Load pretrained BERT parameters\n    bert.load_state_dict(torch.load(os.path.join(data_dir,\n                                                 'pretrained.params')))\n    return bert, vocab\n\ndevices = d2l.try_all_gpus()\nbert, vocab = load_pretrained_model(\n    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n    num_blks=2, dropout=0.1, max_len=512, devices=devices)\n\nclass SNLIBERTDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, max_len, vocab=None):\n        all_premise_hypothesis_tokens = [[\n            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n            *[d2l.tokenize([s.lower() for s in sentences])\n              for sentences in dataset[:2]])]\n\n        self.labels = torch.tensor(dataset[2])\n        self.vocab = vocab\n        self.max_len = max_len\n        (self.all_token_ids, self.all_segments,\n         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n        print('read ' + str(len(self.all_token_ids)) + ' examples')\n\n    def _preprocess(self, all_premise_hypothesis_tokens):\n        pool = multiprocessing.Pool(4)  # Use 4 worker processes\n        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n        all_token_ids = [\n            token_ids for token_ids, segments, valid_len in out]\n        all_segments = [segments for token_ids, segments, valid_len in out]\n        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n        return (torch.tensor(all_token_ids, dtype=torch.long),\n                torch.tensor(all_segments, dtype=torch.long),\n                torch.tensor(valid_lens))\n\n    def _mp_worker(self, premise_hypothesis_tokens):\n        p_tokens, h_tokens = premise_hypothesis_tokens\n        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n                             * (self.max_len - len(tokens))\n        segments = segments + [0] * (self.max_len - len(segments))\n        valid_len = len(tokens)\n        return token_ids, segments, valid_len\n\n    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n        # input\n        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n            if len(p_tokens) > len(h_tokens):\n                p_tokens.pop()\n            else:\n                h_tokens.pop()\n\n    def __getitem__(self, idx):\n        return (self.all_token_ids[idx], self.all_segments[idx],\n                self.valid_lens[idx]), self.labels[idx]\n\n    def __len__(self):\n        return len(self.all_token_ids)\n\n# Reduce `batch_size` if there is an out of memory error. In the original BERT\n# model, `max_len` = 512\nbatch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\ndata_dir = d2l.download_extract('SNLI')\ntrain_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\ntest_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\ntrain_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n                                   num_workers=num_workers)\ntest_iter = torch.utils.data.DataLoader(test_set, batch_size,\n                                  num_workers=num_workers)\n\nclass BERTClassifier(nn.Module):\n    def __init__(self, bert):\n        super(BERTClassifier, self).__init__()\n        self.encoder = bert.encoder\n        self.hidden = bert.hidden\n        self.output = nn.LazyLinear(3)\n\n    def forward(self, inputs):\n        tokens_X, segments_X, valid_lens_x = inputs\n        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n        return self.output(self.hidden(encoded_X[:, 0, :]))\n    \nnet = BERTClassifier(bert)\n\nlr, num_epochs = 1e-4, 5\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction='none')\nnet(next(iter(train_iter))[0])\n\nimport matplotlib.pyplot as plt\n\ndef train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n               devices=d2l.try_all_gpus()):\n    \"\"\"Train a model with multiple GPUs (defined in Chapter 13).\"\"\"\n    timer, num_batches = d2l.Timer(), len(train_iter)\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n                            legend=['train loss', 'train acc', 'test acc'])\n    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n    for epoch in range(num_epochs):\n        # Sum of training loss, sum of training accuracy, no. of examples,\n        # no. of predictions\n        metric = d2l.Accumulator(4)\n        for i, (features, labels) in enumerate(train_iter):\n            timer.start()\n            l, acc = d2l.train_batch_ch13(\n                net, features, labels, loss, trainer, devices)\n            metric.add(l, acc, labels.shape[0], labels.numel())\n            timer.stop()\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (metric[0] / metric[2], metric[1] / metric[3],\n                              None))\n        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch + 1, (None, None, test_acc))\n    \n    # 保存图像\n    plt.savefig('training_progress.png')  # 保存为图片文件\n\n    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n          f'{str(devices)}')\n\n\ntrain_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n\n```","source":"_posts/ml-server-wp.md","raw":"---\ntitle: ml_server_wp\ndate: 2024-12-28 10:26:21\ntags:\n---\n\n\n使用vscode的remote-ssh拓展连接服务器\n\n我不是很确定我是如何搞定这个的，但可能是这样:\n使用shell将conda加入path使其全局可用\n\n创建名为code的目录，然后在vsc中打开它，就可以在vsc的终端中使用它了。\n```shell\nmkdir code\n```\n\n```shell\n# init.sh\n# >>> conda initialize >>>\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('/usr/local/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"/usr/local/miniconda3/etc/profile.d/conda.sh\" ]; then\n        . \"/usr/local/miniconda3/etc/profile.d/conda.sh\"\n    else\n        export PATH=\"/usr/local/miniconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# <<< conda initialize <<\n```\n\n```shell\nsh init.sh\n```\n\n以[某一章节](https://d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html)的代码作为例子。\n\n```shell\nconda create --name d2l python=3.9 -y\nconda activate d2l\npip install torch==2.0.0 torchvision==0.15.1\npip install d2l==1.0.3\n```\n\n不建议使用jupyter，而是以输出图片到相同文件夹下代替\n因此相比原文重写了train_ch13函数\n\n运行成功的代码：\n\n```py\nimport json\nimport multiprocessing\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nd2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n                             '225d66f04cae318b841a13d32af3acc165f253ac')\nd2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n                              'c72329e68a732bef0452e4b96a1c341c8910f81f')\n\ndef load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n                          num_heads, num_blks, dropout, max_len, devices):\n    data_dir = d2l.download_extract(pretrained_model)\n    # Define an empty vocabulary to load the predefined vocabulary\n    vocab = d2l.Vocab()\n    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n        vocab.idx_to_token)}\n    bert = d2l.BERTModel(\n        len(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=4,\n        num_blks=2, dropout=0.2, max_len=max_len)\n    # Load pretrained BERT parameters\n    bert.load_state_dict(torch.load(os.path.join(data_dir,\n                                                 'pretrained.params')))\n    return bert, vocab\n\ndevices = d2l.try_all_gpus()\nbert, vocab = load_pretrained_model(\n    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n    num_blks=2, dropout=0.1, max_len=512, devices=devices)\n\nclass SNLIBERTDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, max_len, vocab=None):\n        all_premise_hypothesis_tokens = [[\n            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n            *[d2l.tokenize([s.lower() for s in sentences])\n              for sentences in dataset[:2]])]\n\n        self.labels = torch.tensor(dataset[2])\n        self.vocab = vocab\n        self.max_len = max_len\n        (self.all_token_ids, self.all_segments,\n         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n        print('read ' + str(len(self.all_token_ids)) + ' examples')\n\n    def _preprocess(self, all_premise_hypothesis_tokens):\n        pool = multiprocessing.Pool(4)  # Use 4 worker processes\n        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n        all_token_ids = [\n            token_ids for token_ids, segments, valid_len in out]\n        all_segments = [segments for token_ids, segments, valid_len in out]\n        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n        return (torch.tensor(all_token_ids, dtype=torch.long),\n                torch.tensor(all_segments, dtype=torch.long),\n                torch.tensor(valid_lens))\n\n    def _mp_worker(self, premise_hypothesis_tokens):\n        p_tokens, h_tokens = premise_hypothesis_tokens\n        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n                             * (self.max_len - len(tokens))\n        segments = segments + [0] * (self.max_len - len(segments))\n        valid_len = len(tokens)\n        return token_ids, segments, valid_len\n\n    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n        # input\n        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n            if len(p_tokens) > len(h_tokens):\n                p_tokens.pop()\n            else:\n                h_tokens.pop()\n\n    def __getitem__(self, idx):\n        return (self.all_token_ids[idx], self.all_segments[idx],\n                self.valid_lens[idx]), self.labels[idx]\n\n    def __len__(self):\n        return len(self.all_token_ids)\n\n# Reduce `batch_size` if there is an out of memory error. In the original BERT\n# model, `max_len` = 512\nbatch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\ndata_dir = d2l.download_extract('SNLI')\ntrain_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\ntest_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\ntrain_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n                                   num_workers=num_workers)\ntest_iter = torch.utils.data.DataLoader(test_set, batch_size,\n                                  num_workers=num_workers)\n\nclass BERTClassifier(nn.Module):\n    def __init__(self, bert):\n        super(BERTClassifier, self).__init__()\n        self.encoder = bert.encoder\n        self.hidden = bert.hidden\n        self.output = nn.LazyLinear(3)\n\n    def forward(self, inputs):\n        tokens_X, segments_X, valid_lens_x = inputs\n        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n        return self.output(self.hidden(encoded_X[:, 0, :]))\n    \nnet = BERTClassifier(bert)\n\nlr, num_epochs = 1e-4, 5\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction='none')\nnet(next(iter(train_iter))[0])\n\nimport matplotlib.pyplot as plt\n\ndef train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n               devices=d2l.try_all_gpus()):\n    \"\"\"Train a model with multiple GPUs (defined in Chapter 13).\"\"\"\n    timer, num_batches = d2l.Timer(), len(train_iter)\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n                            legend=['train loss', 'train acc', 'test acc'])\n    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n    for epoch in range(num_epochs):\n        # Sum of training loss, sum of training accuracy, no. of examples,\n        # no. of predictions\n        metric = d2l.Accumulator(4)\n        for i, (features, labels) in enumerate(train_iter):\n            timer.start()\n            l, acc = d2l.train_batch_ch13(\n                net, features, labels, loss, trainer, devices)\n            metric.add(l, acc, labels.shape[0], labels.numel())\n            timer.stop()\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (metric[0] / metric[2], metric[1] / metric[3],\n                              None))\n        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch + 1, (None, None, test_acc))\n    \n    # 保存图像\n    plt.savefig('training_progress.png')  # 保存为图片文件\n\n    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n          f'{str(devices)}')\n\n\ntrain_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n\n```","slug":"ml-server-wp","published":1,"updated":"2024-12-28T02:26:21.927Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cm57ltacz00029wnq3xbl2hqb","content":"<p>使用vscode的remote-ssh拓展连接服务器</p>\n<p>我不是很确定我是如何搞定这个的，但可能是这样:<br>使用shell将conda加入path使其全局可用</p>\n<p>创建名为code的目录，然后在vsc中打开它，就可以在vsc的终端中使用它了。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir code</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">init.sh</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">&gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">!! Contents within this block are managed by <span class=\"string\">&#x27;conda init&#x27;</span> !!</span></span><br><span class=\"line\">__conda_setup=&quot;$(&#x27;/usr/local/miniconda3/bin/conda&#x27; &#x27;shell.bash&#x27; &#x27;hook&#x27; 2&gt; /dev/null)&quot;</span><br><span class=\"line\">if [ $? -eq 0 ]; then</span><br><span class=\"line\">    eval &quot;$__conda_setup&quot;</span><br><span class=\"line\">else</span><br><span class=\"line\">    if [ -f &quot;/usr/local/miniconda3/etc/profile.d/conda.sh&quot; ]; then</span><br><span class=\"line\">        . &quot;/usr/local/miniconda3/etc/profile.d/conda.sh&quot;</span><br><span class=\"line\">    else</span><br><span class=\"line\">        export PATH=&quot;/usr/local/miniconda3/bin:$PATH&quot;</span><br><span class=\"line\">    fi</span><br><span class=\"line\">fi</span><br><span class=\"line\">unset __conda_setup</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">&lt;&lt;&lt; <span class=\"string\">conda initialize &lt;&lt;</span></span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh init.sh</span><br></pre></td></tr></table></figure>\n\n<p>以<a href=\"https://d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html\">某一章节</a>的代码作为例子。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create --name d2l python=3.9 -y</span><br><span class=\"line\">conda activate d2l</span><br><span class=\"line\">pip install torch==2.0.0 torchvision==0.15.1</span><br><span class=\"line\">pip install d2l==1.0.3</span><br></pre></td></tr></table></figure>\n\n<p>不建议使用jupyter，而是以输出图片到相同文件夹下代替<br>因此相比原文重写了train_ch13函数</p>\n<p>运行成功的代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"><span class=\"keyword\">import</span> multiprocessing</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;bert.base&#x27;</span>] = (d2l.DATA_URL + <span class=\"string\">&#x27;bert.base.torch.zip&#x27;</span>,</span><br><span class=\"line\">                             <span class=\"string\">&#x27;225d66f04cae318b841a13d32af3acc165f253ac&#x27;</span>)</span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;bert.small&#x27;</span>] = (d2l.DATA_URL + <span class=\"string\">&#x27;bert.small.torch.zip&#x27;</span>,</span><br><span class=\"line\">                              <span class=\"string\">&#x27;c72329e68a732bef0452e4b96a1c341c8910f81f&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_pretrained_model</span>(<span class=\"params\">pretrained_model, num_hiddens, ffn_num_hiddens,</span></span><br><span class=\"line\"><span class=\"params\">                          num_heads, num_blks, dropout, max_len, devices</span>):</span><br><span class=\"line\">    data_dir = d2l.download_extract(pretrained_model)</span><br><span class=\"line\">    <span class=\"comment\"># Define an empty vocabulary to load the predefined vocabulary</span></span><br><span class=\"line\">    vocab = d2l.Vocab()</span><br><span class=\"line\">    vocab.idx_to_token = json.load(<span class=\"built_in\">open</span>(os.path.join(data_dir, <span class=\"string\">&#x27;vocab.json&#x27;</span>)))</span><br><span class=\"line\">    vocab.token_to_idx = &#123;token: idx <span class=\"keyword\">for</span> idx, token <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(</span><br><span class=\"line\">        vocab.idx_to_token)&#125;</span><br><span class=\"line\">    bert = d2l.BERTModel(</span><br><span class=\"line\">        <span class=\"built_in\">len</span>(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=<span class=\"number\">4</span>,</span><br><span class=\"line\">        num_blks=<span class=\"number\">2</span>, dropout=<span class=\"number\">0.2</span>, max_len=max_len)</span><br><span class=\"line\">    <span class=\"comment\"># Load pretrained BERT parameters</span></span><br><span class=\"line\">    bert.load_state_dict(torch.load(os.path.join(data_dir,</span><br><span class=\"line\">                                                 <span class=\"string\">&#x27;pretrained.params&#x27;</span>)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> bert, vocab</span><br><span class=\"line\"></span><br><span class=\"line\">devices = d2l.try_all_gpus()</span><br><span class=\"line\">bert, vocab = load_pretrained_model(</span><br><span class=\"line\">    <span class=\"string\">&#x27;bert.small&#x27;</span>, num_hiddens=<span class=\"number\">256</span>, ffn_num_hiddens=<span class=\"number\">512</span>, num_heads=<span class=\"number\">4</span>,</span><br><span class=\"line\">    num_blks=<span class=\"number\">2</span>, dropout=<span class=\"number\">0.1</span>, max_len=<span class=\"number\">512</span>, devices=devices)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SNLIBERTDataset</span>(torch.utils.data.Dataset):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dataset, max_len, vocab=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        all_premise_hypothesis_tokens = [[</span><br><span class=\"line\">            p_tokens, h_tokens] <span class=\"keyword\">for</span> p_tokens, h_tokens <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(</span><br><span class=\"line\">            *[d2l.tokenize([s.lower() <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> sentences])</span><br><span class=\"line\">              <span class=\"keyword\">for</span> sentences <span class=\"keyword\">in</span> dataset[:<span class=\"number\">2</span>]])]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.labels = torch.tensor(dataset[<span class=\"number\">2</span>])</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.vocab = vocab</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.max_len = max_len</span><br><span class=\"line\">        (<span class=\"variable language_\">self</span>.all_token_ids, <span class=\"variable language_\">self</span>.all_segments,</span><br><span class=\"line\">         <span class=\"variable language_\">self</span>.valid_lens) = <span class=\"variable language_\">self</span>._preprocess(all_premise_hypothesis_tokens)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;read &#x27;</span> + <span class=\"built_in\">str</span>(<span class=\"built_in\">len</span>(<span class=\"variable language_\">self</span>.all_token_ids)) + <span class=\"string\">&#x27; examples&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_preprocess</span>(<span class=\"params\">self, all_premise_hypothesis_tokens</span>):</span><br><span class=\"line\">        pool = multiprocessing.Pool(<span class=\"number\">4</span>)  <span class=\"comment\"># Use 4 worker processes</span></span><br><span class=\"line\">        out = pool.<span class=\"built_in\">map</span>(<span class=\"variable language_\">self</span>._mp_worker, all_premise_hypothesis_tokens)</span><br><span class=\"line\">        all_token_ids = [</span><br><span class=\"line\">            token_ids <span class=\"keyword\">for</span> token_ids, segments, valid_len <span class=\"keyword\">in</span> out]</span><br><span class=\"line\">        all_segments = [segments <span class=\"keyword\">for</span> token_ids, segments, valid_len <span class=\"keyword\">in</span> out]</span><br><span class=\"line\">        valid_lens = [valid_len <span class=\"keyword\">for</span> token_ids, segments, valid_len <span class=\"keyword\">in</span> out]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (torch.tensor(all_token_ids, dtype=torch.long),</span><br><span class=\"line\">                torch.tensor(all_segments, dtype=torch.long),</span><br><span class=\"line\">                torch.tensor(valid_lens))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_mp_worker</span>(<span class=\"params\">self, premise_hypothesis_tokens</span>):</span><br><span class=\"line\">        p_tokens, h_tokens = premise_hypothesis_tokens</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>._truncate_pair_of_tokens(p_tokens, h_tokens)</span><br><span class=\"line\">        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)</span><br><span class=\"line\">        token_ids = <span class=\"variable language_\">self</span>.vocab[tokens] + [<span class=\"variable language_\">self</span>.vocab[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>]] \\</span><br><span class=\"line\">                             * (<span class=\"variable language_\">self</span>.max_len - <span class=\"built_in\">len</span>(tokens))</span><br><span class=\"line\">        segments = segments + [<span class=\"number\">0</span>] * (<span class=\"variable language_\">self</span>.max_len - <span class=\"built_in\">len</span>(segments))</span><br><span class=\"line\">        valid_len = <span class=\"built_in\">len</span>(tokens)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> token_ids, segments, valid_len</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_truncate_pair_of_tokens</span>(<span class=\"params\">self, p_tokens, h_tokens</span>):</span><br><span class=\"line\">        <span class=\"comment\"># Reserve slots for &#x27;&lt;CLS&gt;&#x27;, &#x27;&lt;SEP&gt;&#x27;, and &#x27;&lt;SEP&gt;&#x27; tokens for the BERT</span></span><br><span class=\"line\">        <span class=\"comment\"># input</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"built_in\">len</span>(p_tokens) + <span class=\"built_in\">len</span>(h_tokens) &gt; <span class=\"variable language_\">self</span>.max_len - <span class=\"number\">3</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(p_tokens) &gt; <span class=\"built_in\">len</span>(h_tokens):</span><br><span class=\"line\">                p_tokens.pop()</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                h_tokens.pop()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (<span class=\"variable language_\">self</span>.all_token_ids[idx], <span class=\"variable language_\">self</span>.all_segments[idx],</span><br><span class=\"line\">                <span class=\"variable language_\">self</span>.valid_lens[idx]), <span class=\"variable language_\">self</span>.labels[idx]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__len__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(<span class=\"variable language_\">self</span>.all_token_ids)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Reduce `batch_size` if there is an out of memory error. In the original BERT</span></span><br><span class=\"line\"><span class=\"comment\"># model, `max_len` = 512</span></span><br><span class=\"line\">batch_size, max_len, num_workers = <span class=\"number\">512</span>, <span class=\"number\">128</span>, d2l.get_dataloader_workers()</span><br><span class=\"line\">data_dir = d2l.download_extract(<span class=\"string\">&#x27;SNLI&#x27;</span>)</span><br><span class=\"line\">train_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class=\"literal\">True</span>), max_len, vocab)</span><br><span class=\"line\">test_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class=\"literal\">False</span>), max_len, vocab)</span><br><span class=\"line\">train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=<span class=\"literal\">True</span>,</span><br><span class=\"line\">                                   num_workers=num_workers)</span><br><span class=\"line\">test_iter = torch.utils.data.DataLoader(test_set, batch_size,</span><br><span class=\"line\">                                  num_workers=num_workers)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BERTClassifier</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, bert</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(BERTClassifier, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.encoder = bert.encoder</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.hidden = bert.hidden</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.output = nn.LazyLinear(<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, inputs</span>):</span><br><span class=\"line\">        tokens_X, segments_X, valid_lens_x = inputs</span><br><span class=\"line\">        encoded_X = <span class=\"variable language_\">self</span>.encoder(tokens_X, segments_X, valid_lens_x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.output(<span class=\"variable language_\">self</span>.hidden(encoded_X[:, <span class=\"number\">0</span>, :]))</span><br><span class=\"line\">    </span><br><span class=\"line\">net = BERTClassifier(bert)</span><br><span class=\"line\"></span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">1e-4</span>, <span class=\"number\">5</span></span><br><span class=\"line\">trainer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">net(<span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(train_iter))[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch13</span>(<span class=\"params\">net, train_iter, test_iter, loss, trainer, num_epochs,</span></span><br><span class=\"line\"><span class=\"params\">               devices=d2l.try_all_gpus(<span class=\"params\"></span>)</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Train a model with multiple GPUs (defined in Chapter 13).&quot;&quot;&quot;</span></span><br><span class=\"line\">    timer, num_batches = d2l.Timer(), <span class=\"built_in\">len</span>(train_iter)</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, xlim=[<span class=\"number\">1</span>, num_epochs], ylim=[<span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">                            legend=[<span class=\"string\">&#x27;train loss&#x27;</span>, <span class=\"string\">&#x27;train acc&#x27;</span>, <span class=\"string\">&#x27;test acc&#x27;</span>])</span><br><span class=\"line\">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class=\"number\">0</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"comment\"># Sum of training loss, sum of training accuracy, no. of examples,</span></span><br><span class=\"line\">        <span class=\"comment\"># no. of predictions</span></span><br><span class=\"line\">        metric = d2l.Accumulator(<span class=\"number\">4</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i, (features, labels) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_iter):</span><br><span class=\"line\">            timer.start()</span><br><span class=\"line\">            l, acc = d2l.train_batch_ch13(</span><br><span class=\"line\">                net, features, labels, loss, trainer, devices)</span><br><span class=\"line\">            metric.add(l, acc, labels.shape[<span class=\"number\">0</span>], labels.numel())</span><br><span class=\"line\">            timer.stop()</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i + <span class=\"number\">1</span>) % (num_batches // <span class=\"number\">5</span>) == <span class=\"number\">0</span> <span class=\"keyword\">or</span> i == num_batches - <span class=\"number\">1</span>:</span><br><span class=\"line\">                animator.add(epoch + (i + <span class=\"number\">1</span>) / num_batches,</span><br><span class=\"line\">                             (metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>], metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">3</span>],</span><br><span class=\"line\">                              <span class=\"literal\">None</span>))</span><br><span class=\"line\">        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)</span><br><span class=\"line\">        animator.add(epoch + <span class=\"number\">1</span>, (<span class=\"literal\">None</span>, <span class=\"literal\">None</span>, test_acc))</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 保存图像</span></span><br><span class=\"line\">    plt.savefig(<span class=\"string\">&#x27;training_progress.png&#x27;</span>)  <span class=\"comment\"># 保存为图片文件</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;loss <span class=\"subst\">&#123;metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>]:<span class=\"number\">.3</span>f&#125;</span>, train acc &#x27;</span></span><br><span class=\"line\">          <span class=\"string\">f&#x27;<span class=\"subst\">&#123;metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">3</span>]:<span class=\"number\">.3</span>f&#125;</span>, test acc <span class=\"subst\">&#123;test_acc:<span class=\"number\">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;metric[<span class=\"number\">2</span>] * num_epochs / timer.<span class=\"built_in\">sum</span>():<span class=\"number\">.1</span>f&#125;</span> examples/sec on &#x27;</span></span><br><span class=\"line\">          <span class=\"string\">f&#x27;<span class=\"subst\">&#123;<span class=\"built_in\">str</span>(devices)&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>使用vscode的remote-ssh拓展连接服务器</p>\n<p>我不是很确定我是如何搞定这个的，但可能是这样:<br>使用shell将conda加入path使其全局可用</p>\n<p>创建名为code的目录，然后在vsc中打开它，就可以在vsc的终端中使用它了。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir code</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">init.sh</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">&gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">!! Contents within this block are managed by <span class=\"string\">&#x27;conda init&#x27;</span> !!</span></span><br><span class=\"line\">__conda_setup=&quot;$(&#x27;/usr/local/miniconda3/bin/conda&#x27; &#x27;shell.bash&#x27; &#x27;hook&#x27; 2&gt; /dev/null)&quot;</span><br><span class=\"line\">if [ $? -eq 0 ]; then</span><br><span class=\"line\">    eval &quot;$__conda_setup&quot;</span><br><span class=\"line\">else</span><br><span class=\"line\">    if [ -f &quot;/usr/local/miniconda3/etc/profile.d/conda.sh&quot; ]; then</span><br><span class=\"line\">        . &quot;/usr/local/miniconda3/etc/profile.d/conda.sh&quot;</span><br><span class=\"line\">    else</span><br><span class=\"line\">        export PATH=&quot;/usr/local/miniconda3/bin:$PATH&quot;</span><br><span class=\"line\">    fi</span><br><span class=\"line\">fi</span><br><span class=\"line\">unset __conda_setup</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">&lt;&lt;&lt; <span class=\"string\">conda initialize &lt;&lt;</span></span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh init.sh</span><br></pre></td></tr></table></figure>\n\n<p>以<a href=\"https://d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html\">某一章节</a>的代码作为例子。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create --name d2l python=3.9 -y</span><br><span class=\"line\">conda activate d2l</span><br><span class=\"line\">pip install torch==2.0.0 torchvision==0.15.1</span><br><span class=\"line\">pip install d2l==1.0.3</span><br></pre></td></tr></table></figure>\n\n<p>不建议使用jupyter，而是以输出图片到相同文件夹下代替<br>因此相比原文重写了train_ch13函数</p>\n<p>运行成功的代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"><span class=\"keyword\">import</span> multiprocessing</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;bert.base&#x27;</span>] = (d2l.DATA_URL + <span class=\"string\">&#x27;bert.base.torch.zip&#x27;</span>,</span><br><span class=\"line\">                             <span class=\"string\">&#x27;225d66f04cae318b841a13d32af3acc165f253ac&#x27;</span>)</span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;bert.small&#x27;</span>] = (d2l.DATA_URL + <span class=\"string\">&#x27;bert.small.torch.zip&#x27;</span>,</span><br><span class=\"line\">                              <span class=\"string\">&#x27;c72329e68a732bef0452e4b96a1c341c8910f81f&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_pretrained_model</span>(<span class=\"params\">pretrained_model, num_hiddens, ffn_num_hiddens,</span></span><br><span class=\"line\"><span class=\"params\">                          num_heads, num_blks, dropout, max_len, devices</span>):</span><br><span class=\"line\">    data_dir = d2l.download_extract(pretrained_model)</span><br><span class=\"line\">    <span class=\"comment\"># Define an empty vocabulary to load the predefined vocabulary</span></span><br><span class=\"line\">    vocab = d2l.Vocab()</span><br><span class=\"line\">    vocab.idx_to_token = json.load(<span class=\"built_in\">open</span>(os.path.join(data_dir, <span class=\"string\">&#x27;vocab.json&#x27;</span>)))</span><br><span class=\"line\">    vocab.token_to_idx = &#123;token: idx <span class=\"keyword\">for</span> idx, token <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(</span><br><span class=\"line\">        vocab.idx_to_token)&#125;</span><br><span class=\"line\">    bert = d2l.BERTModel(</span><br><span class=\"line\">        <span class=\"built_in\">len</span>(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=<span class=\"number\">4</span>,</span><br><span class=\"line\">        num_blks=<span class=\"number\">2</span>, dropout=<span class=\"number\">0.2</span>, max_len=max_len)</span><br><span class=\"line\">    <span class=\"comment\"># Load pretrained BERT parameters</span></span><br><span class=\"line\">    bert.load_state_dict(torch.load(os.path.join(data_dir,</span><br><span class=\"line\">                                                 <span class=\"string\">&#x27;pretrained.params&#x27;</span>)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> bert, vocab</span><br><span class=\"line\"></span><br><span class=\"line\">devices = d2l.try_all_gpus()</span><br><span class=\"line\">bert, vocab = load_pretrained_model(</span><br><span class=\"line\">    <span class=\"string\">&#x27;bert.small&#x27;</span>, num_hiddens=<span class=\"number\">256</span>, ffn_num_hiddens=<span class=\"number\">512</span>, num_heads=<span class=\"number\">4</span>,</span><br><span class=\"line\">    num_blks=<span class=\"number\">2</span>, dropout=<span class=\"number\">0.1</span>, max_len=<span class=\"number\">512</span>, devices=devices)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SNLIBERTDataset</span>(torch.utils.data.Dataset):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dataset, max_len, vocab=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        all_premise_hypothesis_tokens = [[</span><br><span class=\"line\">            p_tokens, h_tokens] <span class=\"keyword\">for</span> p_tokens, h_tokens <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(</span><br><span class=\"line\">            *[d2l.tokenize([s.lower() <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> sentences])</span><br><span class=\"line\">              <span class=\"keyword\">for</span> sentences <span class=\"keyword\">in</span> dataset[:<span class=\"number\">2</span>]])]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.labels = torch.tensor(dataset[<span class=\"number\">2</span>])</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.vocab = vocab</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.max_len = max_len</span><br><span class=\"line\">        (<span class=\"variable language_\">self</span>.all_token_ids, <span class=\"variable language_\">self</span>.all_segments,</span><br><span class=\"line\">         <span class=\"variable language_\">self</span>.valid_lens) = <span class=\"variable language_\">self</span>._preprocess(all_premise_hypothesis_tokens)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;read &#x27;</span> + <span class=\"built_in\">str</span>(<span class=\"built_in\">len</span>(<span class=\"variable language_\">self</span>.all_token_ids)) + <span class=\"string\">&#x27; examples&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_preprocess</span>(<span class=\"params\">self, all_premise_hypothesis_tokens</span>):</span><br><span class=\"line\">        pool = multiprocessing.Pool(<span class=\"number\">4</span>)  <span class=\"comment\"># Use 4 worker processes</span></span><br><span class=\"line\">        out = pool.<span class=\"built_in\">map</span>(<span class=\"variable language_\">self</span>._mp_worker, all_premise_hypothesis_tokens)</span><br><span class=\"line\">        all_token_ids = [</span><br><span class=\"line\">            token_ids <span class=\"keyword\">for</span> token_ids, segments, valid_len <span class=\"keyword\">in</span> out]</span><br><span class=\"line\">        all_segments = [segments <span class=\"keyword\">for</span> token_ids, segments, valid_len <span class=\"keyword\">in</span> out]</span><br><span class=\"line\">        valid_lens = [valid_len <span class=\"keyword\">for</span> token_ids, segments, valid_len <span class=\"keyword\">in</span> out]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (torch.tensor(all_token_ids, dtype=torch.long),</span><br><span class=\"line\">                torch.tensor(all_segments, dtype=torch.long),</span><br><span class=\"line\">                torch.tensor(valid_lens))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_mp_worker</span>(<span class=\"params\">self, premise_hypothesis_tokens</span>):</span><br><span class=\"line\">        p_tokens, h_tokens = premise_hypothesis_tokens</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>._truncate_pair_of_tokens(p_tokens, h_tokens)</span><br><span class=\"line\">        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)</span><br><span class=\"line\">        token_ids = <span class=\"variable language_\">self</span>.vocab[tokens] + [<span class=\"variable language_\">self</span>.vocab[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>]] \\</span><br><span class=\"line\">                             * (<span class=\"variable language_\">self</span>.max_len - <span class=\"built_in\">len</span>(tokens))</span><br><span class=\"line\">        segments = segments + [<span class=\"number\">0</span>] * (<span class=\"variable language_\">self</span>.max_len - <span class=\"built_in\">len</span>(segments))</span><br><span class=\"line\">        valid_len = <span class=\"built_in\">len</span>(tokens)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> token_ids, segments, valid_len</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_truncate_pair_of_tokens</span>(<span class=\"params\">self, p_tokens, h_tokens</span>):</span><br><span class=\"line\">        <span class=\"comment\"># Reserve slots for &#x27;&lt;CLS&gt;&#x27;, &#x27;&lt;SEP&gt;&#x27;, and &#x27;&lt;SEP&gt;&#x27; tokens for the BERT</span></span><br><span class=\"line\">        <span class=\"comment\"># input</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"built_in\">len</span>(p_tokens) + <span class=\"built_in\">len</span>(h_tokens) &gt; <span class=\"variable language_\">self</span>.max_len - <span class=\"number\">3</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(p_tokens) &gt; <span class=\"built_in\">len</span>(h_tokens):</span><br><span class=\"line\">                p_tokens.pop()</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                h_tokens.pop()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (<span class=\"variable language_\">self</span>.all_token_ids[idx], <span class=\"variable language_\">self</span>.all_segments[idx],</span><br><span class=\"line\">                <span class=\"variable language_\">self</span>.valid_lens[idx]), <span class=\"variable language_\">self</span>.labels[idx]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__len__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(<span class=\"variable language_\">self</span>.all_token_ids)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Reduce `batch_size` if there is an out of memory error. In the original BERT</span></span><br><span class=\"line\"><span class=\"comment\"># model, `max_len` = 512</span></span><br><span class=\"line\">batch_size, max_len, num_workers = <span class=\"number\">512</span>, <span class=\"number\">128</span>, d2l.get_dataloader_workers()</span><br><span class=\"line\">data_dir = d2l.download_extract(<span class=\"string\">&#x27;SNLI&#x27;</span>)</span><br><span class=\"line\">train_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class=\"literal\">True</span>), max_len, vocab)</span><br><span class=\"line\">test_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class=\"literal\">False</span>), max_len, vocab)</span><br><span class=\"line\">train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=<span class=\"literal\">True</span>,</span><br><span class=\"line\">                                   num_workers=num_workers)</span><br><span class=\"line\">test_iter = torch.utils.data.DataLoader(test_set, batch_size,</span><br><span class=\"line\">                                  num_workers=num_workers)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BERTClassifier</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, bert</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(BERTClassifier, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.encoder = bert.encoder</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.hidden = bert.hidden</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.output = nn.LazyLinear(<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, inputs</span>):</span><br><span class=\"line\">        tokens_X, segments_X, valid_lens_x = inputs</span><br><span class=\"line\">        encoded_X = <span class=\"variable language_\">self</span>.encoder(tokens_X, segments_X, valid_lens_x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.output(<span class=\"variable language_\">self</span>.hidden(encoded_X[:, <span class=\"number\">0</span>, :]))</span><br><span class=\"line\">    </span><br><span class=\"line\">net = BERTClassifier(bert)</span><br><span class=\"line\"></span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">1e-4</span>, <span class=\"number\">5</span></span><br><span class=\"line\">trainer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">net(<span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(train_iter))[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch13</span>(<span class=\"params\">net, train_iter, test_iter, loss, trainer, num_epochs,</span></span><br><span class=\"line\"><span class=\"params\">               devices=d2l.try_all_gpus(<span class=\"params\"></span>)</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Train a model with multiple GPUs (defined in Chapter 13).&quot;&quot;&quot;</span></span><br><span class=\"line\">    timer, num_batches = d2l.Timer(), <span class=\"built_in\">len</span>(train_iter)</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, xlim=[<span class=\"number\">1</span>, num_epochs], ylim=[<span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">                            legend=[<span class=\"string\">&#x27;train loss&#x27;</span>, <span class=\"string\">&#x27;train acc&#x27;</span>, <span class=\"string\">&#x27;test acc&#x27;</span>])</span><br><span class=\"line\">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class=\"number\">0</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"comment\"># Sum of training loss, sum of training accuracy, no. of examples,</span></span><br><span class=\"line\">        <span class=\"comment\"># no. of predictions</span></span><br><span class=\"line\">        metric = d2l.Accumulator(<span class=\"number\">4</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i, (features, labels) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_iter):</span><br><span class=\"line\">            timer.start()</span><br><span class=\"line\">            l, acc = d2l.train_batch_ch13(</span><br><span class=\"line\">                net, features, labels, loss, trainer, devices)</span><br><span class=\"line\">            metric.add(l, acc, labels.shape[<span class=\"number\">0</span>], labels.numel())</span><br><span class=\"line\">            timer.stop()</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i + <span class=\"number\">1</span>) % (num_batches // <span class=\"number\">5</span>) == <span class=\"number\">0</span> <span class=\"keyword\">or</span> i == num_batches - <span class=\"number\">1</span>:</span><br><span class=\"line\">                animator.add(epoch + (i + <span class=\"number\">1</span>) / num_batches,</span><br><span class=\"line\">                             (metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>], metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">3</span>],</span><br><span class=\"line\">                              <span class=\"literal\">None</span>))</span><br><span class=\"line\">        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)</span><br><span class=\"line\">        animator.add(epoch + <span class=\"number\">1</span>, (<span class=\"literal\">None</span>, <span class=\"literal\">None</span>, test_acc))</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 保存图像</span></span><br><span class=\"line\">    plt.savefig(<span class=\"string\">&#x27;training_progress.png&#x27;</span>)  <span class=\"comment\"># 保存为图片文件</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;loss <span class=\"subst\">&#123;metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>]:<span class=\"number\">.3</span>f&#125;</span>, train acc &#x27;</span></span><br><span class=\"line\">          <span class=\"string\">f&#x27;<span class=\"subst\">&#123;metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">3</span>]:<span class=\"number\">.3</span>f&#125;</span>, test acc <span class=\"subst\">&#123;test_acc:<span class=\"number\">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;metric[<span class=\"number\">2</span>] * num_epochs / timer.<span class=\"built_in\">sum</span>():<span class=\"number\">.1</span>f&#125;</span> examples/sec on &#x27;</span></span><br><span class=\"line\">          <span class=\"string\">f&#x27;<span class=\"subst\">&#123;<span class=\"built_in\">str</span>(devices)&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}